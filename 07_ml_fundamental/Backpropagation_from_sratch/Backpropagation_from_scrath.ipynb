{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M54VIPsiIKfL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "### we need to define the activation function\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1+  np.exp(-z))\n",
        "\n",
        "## Now we need to find the derivetive of  relu and segmoid\n",
        "def relu_derivative(z):\n",
        "    return(z>0).astype(float)\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    a = sigmoid(z)\n",
        "\n",
        "    return a * (1-a)\n",
        "\n",
        "\n",
        "# Now we can define the forword propagation\n",
        "class TinyProp_function:\n",
        "    def __init__(self, input_dim =None, hidden_dim = None, output_dim = None, learning_rate = 0.1 ):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        # we will work with  One hidden layer\n",
        "\n",
        "        self.w1 = np.random.randn(input_dim, hidden_dim)\n",
        "        self.b1 = np.zeros((1, hidden_dim))\n",
        "        self.w2 = np.random.randn(hidden_dim, output_dim)\n",
        "        self.b2 = np.zeros((1, output_dim))\n",
        "\n",
        "#  Now we need to do the forward propagation\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "\n",
        "        # hidden layer linear\n",
        "        self.Z1 = self.X @ self.w1 + self.b1\n",
        "        # activation\n",
        "        self.A1 = relu(self.Z1)\n",
        "\n",
        "        # output layer\n",
        "        self.Z2 = self.A1 @ self.w2 + self.b2\n",
        "\n",
        "        # activation function\n",
        "        self.A2 = sigmoid(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    # After this we need to compute the loos\n",
        "    def compute_loos(self, y):\n",
        "        n = y.shape[0] # n_sample\n",
        "        loss = np.mean ((self.A2- y)**2)\n",
        "        return loss\n",
        "\n",
        "    ''' Now that we compute the loss we need to go back doing back propagation to correct our error,\n",
        "    to do that we need to do the derivative from output to input which is gradient '''\n",
        "\n",
        "    # now we have to do back propagation now\n",
        "    def backprop(self, y):\n",
        "        n = y.shape[0]\n",
        "        # derivative of A2 according to the loos\n",
        "        # dL/ dA2 = 2 * (self.A2 - y)/n\n",
        "        dA2 = 2 * (self.A2 - y)/n\n",
        "\n",
        "        # Now dA2/ dZ2\n",
        "        dZ2 = dA2 * sigmoid_derivative(self.A2)\n",
        "\n",
        "        # Z2 = xW2 + b2\n",
        "        dw2 = self.A1.T  @ dZ2\n",
        "\n",
        "        db2 = np.sum(dZ2, axis= 0, keepdims = True)\n",
        "\n",
        "        # now A1\n",
        "        dA1 = dZ2 @ self.w2.T\n",
        "\n",
        "        # let do dz1\n",
        "        dZ1 = dA1 * relu_derivative(self.Z1)\n",
        "\n",
        "        dw1 = self.X.T @ dZ1\n",
        "\n",
        "        db1 = np.sum(dZ1, axis= 0, keepdims= True)\n",
        "\n",
        "        # now we need to update weight and bias\n",
        "        lr = self.learning_rate\n",
        "        self.w2 -= lr * dw2\n",
        "        self.b2 -= lr * db2\n",
        "        self.w1 -= lr * dw1\n",
        "        self.w1 -= lr * db1\n",
        "\n",
        "    # do the training step\n",
        "    # here we call forward, loss and back function\n",
        "    def train_step(self, X, y):\n",
        "        #  forward\n",
        "        first_pred = self.forward(X)\n",
        "\n",
        "        # loos\n",
        "        loss = self.compute_loos(y)\n",
        "\n",
        "        # backward(Update parameter)\n",
        "        self.backprop(y)\n",
        "        return loss, first_pred\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### text code"
      ],
      "metadata": {
        "id": "xU9CYUbnIuk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    X = np.array([\n",
        "        [0.0, 0.0],\n",
        "        [0.0, 1.0],\n",
        "        [1.0, 0.0],\n",
        "        [1.0, 1.0],\n",
        "    ])\n",
        "\n",
        "    y = np.array([\n",
        "        [0.0],\n",
        "        [1.0],\n",
        "        [1.0],\n",
        "        [0.0],\n",
        "    ])\n",
        "\n",
        "    net = TinyProp_function(input_dim= 2, hidden_dim= 3, output_dim= 1, learning_rate= 0.1)\n",
        "\n",
        "    for epoch in range (1000):\n",
        "        loss, first_pred = net.train_step(X, y)\n",
        "        if (epoch + 1) % 100 ==0:\n",
        "            print(f'Epoch {epoch+1}, Loss = {loss:.4f}')\n",
        "    print(\"\\nFinal predictions:\")\n",
        "    print(first_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPVZuwl6IWEa",
        "outputId": "2f70a30e-f271-454e-e056-c0749df80b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss = 0.1610\n",
            "Epoch 200, Loss = 0.1137\n",
            "Epoch 300, Loss = 0.0720\n",
            "Epoch 400, Loss = 0.0431\n",
            "Epoch 500, Loss = 0.0262\n",
            "Epoch 600, Loss = 0.0168\n",
            "Epoch 700, Loss = 0.0113\n",
            "Epoch 800, Loss = 0.0079\n",
            "Epoch 900, Loss = 0.0058\n",
            "Epoch 1000, Loss = 0.0044\n",
            "\n",
            "Final predictions:\n",
            "[[0.08573381]\n",
            " [0.96655047]\n",
            " [0.95890866]\n",
            " [0.08573381]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rUQWHO--IZCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}