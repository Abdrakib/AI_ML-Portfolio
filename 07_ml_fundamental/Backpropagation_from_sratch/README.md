# Backpropagation from Scratch

## Description
This project implements backpropagation for a neural network from scratch
without using high-level deep learning libraries.

The goal is to understand how neural networks learn by manually implementing
forward propagation, loss calculation, gradient computation, and parameter
updates.

## What This Project Covers
- Forward propagation
- Loss function computation
- Backpropagation (gradient calculation)
- Weight and bias updates using gradient descent
- Training loop implementation

## Approach
1. Initialize weights and biases manually
2. Perform forward propagation to compute predictions
3. Calculate the loss between predictions and true labels
4. Compute gradients using backpropagation
5. Update parameters using gradient descent
6. Repeat for multiple epochs

## Key Concepts
- Neural networks fundamentals
- Gradient descent optimization
- Chain rule in backpropagation
- Model training dynamics

## Files
- `Backpropagation_from_scrath.ipynb` â€” complete implementation of
  backpropagation from scratch

## Tools & Libraries
- Python
- NumPy

## How to Run
1. Open the notebook:
   ```bash
   Backpropagation_from_scrath.ipynb
